{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream data to from Kafka to Cosmos DB\n",
    "\n",
    "Spark Structured Streaming to retrieve data from Kafka on HDInsight and store it into Azure Cosmos DB. It uses the [Azure CosmosDB Spark Connector] to write to a Cosmos DB CQL/SQL API database.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "* An Azure Virtual Network\n",
    "* A Spark on HDInsight 3.6 cluster, inside the virtual network\n",
    "* A Kafka on HDInsight cluster, inside the virtual network\n",
    "* A Cosmos DB SQL API database\n",
    "\n",
    "## Load packages\n",
    "\n",
    "* spark-sql-kafka-0-10_2.11, version 2.1.0 - Used to read from Kafka.\n",
    "* azure-cosmosdb-spark_2.1.0_2.11, version 1.0.0 - The Spark connector used to communicate with Azure Cosmos DB.\n",
    "* azure-documentdb, version 1.15.1 - The DocumentDB SDK. This is used by the connector to communicate with Cosmos DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{u'kind': 'spark', u'name': u'Spark-to-Cosmos_DB_Connector', u'driverMemory': u'2G', u'numExecutors': 9, u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,com.microsoft.azure:azure-cosmosdb-spark_2.2.0_2.11:1.0.0,com.microsoft.azure:azure-documentdb:1.15.1', u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}, u'executorCores': 2, u'executorMemory': u'8G'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7</td><td>application_1543170503042_0008</td><td>spark</td><td>busy</td><td><a target=\"_blank\" href=\"http://hn1-spark.yr21tghw1lbedojd2c1yzdkfqb.dx.internal.cloudapp.net:8088/proxy/application_1543170503042_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn3-spark.yr21tghw1lbedojd2c1yzdkfqb.dx.internal.cloudapp.net:30060/node/containerlogs/container_e02_1543170503042_0008_01_000001/livy\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"name\":\"Spark-to-Cosmos_DB_Connector\", \n",
    "    \"executorMemory\": \"8G\", \n",
    "    \"executorCores\": 2, \n",
    "    \"numExecutors\":9,\n",
    "    \"driverMemory\" : \"2G\",\n",
    "    \"conf\": {\n",
    "        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,com.microsoft.azure:azure-cosmosdb-spark_2.2.0_2.11:1.0.0,com.microsoft.azure:azure-documentdb:1.15.1\", \n",
    "        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Kafka broker hosts information\n",
    "\"wn0-kafka.yr21tghw1lbedojd2c1yzdkfqb.dx.internal.cloudapp.net:9092,wn1-kafka.yr21tghw1lbedojd2c1yzdkfqb.dx.internal.cloudapp.net:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8</td><td>application_1543170503042_0009</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-spark.yr21tghw1lbedojd2c1yzdkfqb.dx.internal.cloudapp.net:8088/proxy/application_1543170503042_0009/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn3-spark.yr21tghw1lbedojd2c1yzdkfqb.dx.internal.cloudapp.net:30060/node/containerlogs/container_e02_1543170503042_0009_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "broker and topic set."
     ]
    }
   ],
   "source": [
    "// The Kafka broker hosts and topic used to read to Kafka\n",
    "val kafkaBrokers=\"wn0-kafka.yr21tghw1lbedojd2c1yzdkfqb.dx.internal.cloudapp.net:9092,wn1-kafka.yr21tghw1lbedojd2c1yzdkfqb.dx.internal.cloudapp.net:9092\"\n",
    "val kafkaTopic=\"trafficdata\"\n",
    "\n",
    "println(\"broker and topic set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Cosmos DB connection information\n",
    "\n",
    "Using the information in [Create a document database using Java and the Azure portal](https://docs.microsoft.com/en-us/azure/cosmos-db/create-sql-api-java) we create a database and collection, then retrieve the endpoint, master key, and preferred region information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosmos DB configuration set."
     ]
    }
   ],
   "source": [
    "// Import Necessary Libraries\n",
    "import org.joda.time._\n",
    "import org.joda.time.format._\n",
    "\n",
    "// Current version of the connector\n",
    "import com.microsoft.azure.cosmosdb.spark.schema._\n",
    "import com.microsoft.azure.cosmosdb.spark._\n",
    "import com.microsoft.azure.cosmosdb.spark.streaming.CosmosDBSinkProvider\n",
    "import com.microsoft.azure.cosmosdb.spark.config.Config\n",
    "\n",
    "var configMap = Map(\n",
    "    \"Endpoint\" -> \"https://bigdatanew.documents.azure.com:443/\",\n",
    "    \"Masterkey\" -> \"NLDkyJR9hDdRs4eexlqXvKPUVUCM5YuXebHrhQj9kkNpZg1pMXF3731FLSmPqEo7vDWuqAuzKJs3e8p7CtvDkA==\",\n",
    "    \"Database\" -> \"trafficdata\",\n",
    "    // use a ';' to delimit multiple regions\n",
    "    \"PreferredRegions\" -> \"West US;\",\n",
    "    \"Collection\" -> \"trafficcollection\"\n",
    ")\n",
    "\n",
    "println(\"Cosmos DB configuration set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cassandra API configuration in Spark2 \n",
    "\n",
    "Using the given instruction in https://docs.microsoft.com/en-us/azure/cosmos-db/cassandra-spark-hdinsight\n",
    "The Spark connector for Cassandra requires that the Cassandra connection details to be initialized as part of the Spark context. \n",
    "When you launch a Jupyter notebook, the spark session and context are already initialized and it is not advisable to stop and reinitialize the Spark context unless it's complete with every configuration set as part of the HDInsight default Jupyter notebook start-up. One workaround is to add the Cassandra instance details to Ambari, Spark2 service configuration directly. This is a one-time activity per cluster that requires a Spark2 service restart.\n",
    "\n",
    "1.Go to Ambari, Spark2 service and select configs\n",
    "\n",
    "2.Then go to custom spark2-defaults and add a new property with the following, and restart Spark2 service.\n",
    "\n",
    "spark.cassandra.connection.host=YOUR_COSMOSDB_ACCOUNT_NAME.cassandra.cosmosdb.azure.com<br>\n",
    "spark.cassandra.connection.port=10350<br>\n",
    "spark.cassandra.connection.ssl.enabled=true<br>\n",
    "spark.cassandra.auth.username=YOUR_COSMOSDB_ACCOUNT_NAME<br>\n",
    "spark.cassandra.auth.password=YOUR_COSMOSDB_KEY<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package com.microsoft.azure.cosmosdb.cassandra\n",
    "\n",
    "import com.datastax.spark.connector._\n",
    "import com.datastax.spark.connector.cql.CassandraConnector\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "import scala.util.Random\n",
    "\n",
    "  // MAIN\n",
    "  def main (arg: Array[String]): Unit = {\n",
    "\n",
    "    // CONFIG. NOTE: Please read the README.md for more details regarding each conf value.\n",
    "    val conf = new SparkConf(true)\n",
    "      .setAppName(\"SampleCosmosDBCassandraApp\")\n",
    "      // Cosmos DB Cassandra API Connection configs\n",
    "      .set(\"spark.cassandra.connection.host\", \"<COSMOSDB_CASSANDRA_ENDPOINT>\")\n",
    "      .set(\"spark.cassandra.connection.port\", \"10350\")\n",
    "      .set(\"spark.cassandra.connection.ssl.enabled\", \"true\")\n",
    "      .set(\"spark.cassandra.auth.username\", \"COSMOSDB_ACCOUNTNAME\")\n",
    "      .set(\"spark.cassandra.auth.password\", \"COSMODB_KEY\")\n",
    "      // Parallelism and throughput configs.\n",
    "      .set(\"spark.cassandra.output.batch.size.rows\", \"1\")\n",
    "      // NOTE: The values below are meant as defaults for a sample workload. Please read the README.md for more information on fine tuning these conf value.\n",
    "      .set(\"spark.cassandra.connection.connections_per_executor_max\", \"10\")\n",
    "      .set(\"spark.cassandra.output.concurrent.writes\", \"100\")\n",
    "      .set(\"spark.cassandra.concurrent.reads\", \"512\")\n",
    "      .set(\"spark.cassandra.output.batch.grouping.buffer.size\", \"1000\")\n",
    "      .set(\"spark.cassandra.connection.keep_alive_ms\", \"60000\")\n",
    "      // Cosmos DB Connection Factory, configured with retry policy for rate limiting.\n",
    "      .set(\"spark.cassandra.connection.factory\", \"com.microsoft.azure.cosmosdb.CosmosDbConnectionFactory\")\n",
    "\n",
    "\n",
    "    // SPARK CONTEXT\n",
    "    val sc = new SparkContext(conf)\n",
    "\n",
    "    // CREATE KEYSPACE/TABLE, AND ANY ARBITRARY QUERY STRING.\n",
    "    CassandraConnector(conf).withSessionDo { session =>\n",
    "      session.execute(\"CREATE KEYSPACE kspc WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1 }\")\n",
    "      session.execute(\"CREATE TABLE kspc.tble (id1 int, id2 int, col1 int, col2 text, PRIMARY KEY(id1, id2))\")\n",
    "    }\n",
    "\n",
    "    // INSERT DATA\n",
    "    val collection = sc.parallelize(Seq((1, 1, 1, \"text_1\"), (2, 2, 2, \"text_2\")))\n",
    "    collection.saveToCassandra(\"kspc\", \"tble\", SomeColumns(\"id1\", \"id2\", \"col1\", \"col2\"))\n",
    "\n",
    "    // INSERT GENERATED DATA\n",
    "    randomDataPerPartitionId(sc, DoP= 10, start = 0, end = 10000, col1 = 100000, col2 = 100000).\n",
    "      saveToCassandra(\"large\", \"large\", SomeColumns(\"id1\", \"id2\", \"col1\", \"col2\"))\n",
    "      \n",
    "    sc.stop\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the schema and source stream\n",
    "\n",
    "The following cell creates the stream that reads from Kafka. Data read from Kafka contains several columns. In this case, we only use the `value` column, as it contains the taxi trip data written by the other notebook. To make this data easier to work with, a schema is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trafficData: org.apache.spark.sql.DataFrame = [traffic: struct<_direction: string, _fromst: string ... 11 more fields>]"
     ]
    }
   ],
   "source": [
    "// Import bits useed for declaring schemas and working with JSON data\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// Define a schema for the data\n",
    "val schema = (new StructType).add(\"_direction\", StringType).add(\"_fromst\", StringType).add(\"_last_updt\",StringType).add(\"_length\",StringType).add(\"_lif_lat\",StringType).add(\"_lit_lat\",StringType).add(\"_lit_lon\",StringType).add(\"_strheading\",StringType).add(\"_tost\",StringType).add(\"_traffic\",StringType).add(\"segmentid\",StringType).add(\"start_lon\",StringType).add(\"street\",StringType)\n",
    "// Reproduced here for readability\n",
    "//val schema = (new StructType)\n",
    "//   .add(\"_direction\", StringType)\n",
    "//   .add(\"_fromost\", StringType)\n",
    "//   .add(\"_last_updt\", StringType)\n",
    "//   .add(\"_length\", StringType)\n",
    "//   .add(\"_lif_lat\",StringType)\n",
    "//   .add(\"_lit_lat\",StringType)\n",
    "//   .add(\"_lit_lon\",StringType)\n",
    "//   .add(\"_strheading\",StringType)\n",
    "//   .add(\"_tost\",StringType)\n",
    "//   .add(\"_traffic\",StringType)\n",
    "//   .add(\"segmentid\",StringType)\n",
    "//   .add(\"start_lon\",StringType)\n",
    "//   .add(\"street\",StringType)\n",
    "// Reproduced here for readability\n",
    "\n",
    "// Read from the Kafka stream source\n",
    "val kafka = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafkaBrokers).option(\"subscribe\", kafkaTopic).option(\"startingOffsets\",\"earliest\").load()\n",
    "\n",
    "// Select the value of the Kafka message and apply the trip schema to it\n",
    "val trafficData = kafka.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema) as \"traffic\")\n",
    "\n",
    "// The output of this cell is similar to the following value:\n",
    "// taxiData: org.apache.spark.sql.DataFrame = [traffic: struct<_direction: string, _fromst: string ... 11 more fields>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the data to Cosmos DB\n",
    "\n",
    "The following cell selects the traffic data from the stream and writes it to Cosmos DB. This is the data structure that was created in the previous cell by applying a schema to the value data retrieved from kafka.\n",
    "\n",
    "This stream only runs for 10 seconds (10000ms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream finished."
     ]
    }
   ],
   "source": [
    "trafficData.select(\"traffic\").writeStream.format(classOf[CosmosDBSinkProvider].getName).outputMode(\"append\").options(configMap).option(\"checkpointLocation\", \"cosmoscheckpointlocation\").start.awaitTermination(10000)\n",
    "println(\"Stream finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## To verify that data is in Cosmos DB\n",
    "\n",
    "In the [Azure portal](https://portal.azure.com), select your Cosmos DB account, and then select __Data Explorer__. From the dropdown, select the database and collection that the data is written to.Select __Refresh__ before the data appears. Select the id of one of the entries to view the data in Cosmos DB. The document contain data similar to the following:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"traffic\": {\n",
    "    \"_direction\":\"EB\",\n",
    "    \"_fromst\":\"Pulaski\",\n",
    "    \"_last_updt\":\"2018-12-01 01:10:18.0\",\n",
    "    \"_length\":\"0.5\",\n",
    "    \"_lif_lat\":\"41.7930671862\",\n",
    "    \"_lit_lon\":\"-87.7136071496\",\n",
    "    \"_strheading\":\"W\",\n",
    "    \"_tost\":\"Central Park\",\n",
    "    \"_traffic\":\"-1\",\n",
    "    \"segmentid\":\"1\",\n",
    "    \"start_lon\":\"-87.7231602513\",\n",
    "    \"street\":\"55th\",\n",
    "  },\n",
    "  \"id\": \"abfe6ff1-51a7-46a6-9600-1c330166cf12\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect Power BI and Azure  Cosmos DB to create the dashboard\n",
    "\n",
    "1.Run Power BI Desktop.\n",
    "2.You can Get Data, see Recent Sources, or Open Other Reports directly from the welcome screen. \n",
    "  Select the \"X\" at the top right corner to close the screen. \n",
    "3.The Report view of Power BI Desktop is displayed.\n",
    "4.Select the Home ribbon, then click on Get Data. The Get Data window should appear.\n",
    "5.Click on Azure, select Azure Cosmos DB (Beta), and then click Connect.\n",
    "6.On the Preview Connector page, click Continue. The Azure Cosmos DB window appears.\n",
    "7.Specify the Azure Cosmos DB account endpoint URL to retrieve the data and then click OK. \n",
    "8.Retrieve the URL from the URI box in the Keys blade of the Azure portal.\n",
    "9.When the account is successfully connected, the Navigator pane appears. The Navigator shows a list of databases under the account.\n",
    "10.Populated data from Cosmos Db to create the dashboard in Power BI and is scheduled for refresh."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
